{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeluB0XWub2x",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "**<h1><div style=\"text-align: center;\"> INM707 Phomenes Environment </div></h1>**\n",
    "**<h2><div style=\"text-align: center;\"> Assigment: Creating a learning environment and policies </div></h2>**\n",
    "\n",
    "\n",
    "**<h3><div> Game Description</div></h3>**\n",
    "\n",
    "The agent needs to collect words with the short sound 'ʊ' and  find the shorter path to the goal within a limited time. The agent gets a positive reward if collecting the words with the correct sound. A negative reward is given for collecting the incorrect word, hitting the walls, hitting moving obstacles and dragging in empty cells. Each time step has a penalization of -1.\n",
    "\n",
    "The environment has the same number of words for each of the sound :'ʊ' (short u sound),  ʌ (open-middle set) and uː (closed long u). These together with moving obstacles will challenge the agent to achieve the goal.\n",
    "\n",
    "\n",
    "**<h3><div> Environment</div></h3>**\n",
    "\n",
    "The phoneme environment is a configurable $N\\times M$ array of integers representing objects.\n",
    "All objects except the wall are placed randomly in the environment. Each object is represented as follows:\n",
    "\n",
    "- 0 : empty cell\n",
    "- 1 : moving obstacle\n",
    "- 2 : 'ʊ' word\n",
    "- 3 : 'ʌ' word\n",
    "- 4 : 'u:' word\n",
    "- 5 : Agent\n",
    "- 6 : Goal\n",
    "- 7 : Boundaries/walls\n",
    "\n",
    "\n",
    "The words are randomly extracted from the phonemes lists. The grid can be adapted to collect the three sounds or any of their combinations by a minimal change in the rewards and policies functions. For a more advanced task each word with the same sound can be encoded with its own number. In this work the mission is to collect/learn the phonetic sound 'ʊ'.\n",
    "\n",
    "\n",
    "- The available area to placed objects is  total grid area - the boundary area\n",
    "\n",
    "$ a = M\\times N - 2 \\times (M + N) - 4$\n",
    "\n",
    "\n",
    "- The total number of words on the grid is given by floor division of the are by 3 (3 phonemes sounds):\n",
    "\n",
    "$ w  = \\lfloor \\frac{a}{3} \\rfloor $\n",
    "\n",
    "\n",
    "- The words per soud is given by:\n",
    "\n",
    "\n",
    "$  w_i = \\lfloor \\frac{w}{3} \\rfloor $\n",
    "\n",
    "\n",
    "- The number of obstacles on the grid is:\n",
    "\n",
    "\n",
    "$ o = \\lfloor \\frac{a}{9} \\rfloor $\n",
    "\n",
    "\n",
    "- There is only one goal (G) and one learner (A).\n",
    "\n",
    "There are $n+1$ agents on the board, $a_0,…,a_n$, where $a_0$ is the learner agent and the rest are the movable obstacles. \n",
    "\n",
    "\n",
    "**<h3><div> Actions</div></h3>**\n",
    "\n",
    "The actions available at each time step are:\n",
    "- up\n",
    "- down\n",
    "- left \n",
    "- right\n",
    "- grab \n",
    "After taking an action, the agent gets a reward and transitions to a new state. Then the environment sends a signal indicating whether the game is over or not. \n",
    "\n",
    "**<h3><div> Observations</div></h3>**\n",
    "\n",
    "The observation of the environment is a dictionary that contains:\n",
    "- relative coordinates to all words in the grid\n",
    "- relative coordinates to the goal \n",
    "- relative coordinates to the obstacles\n",
    "- a neighbourhood 3x3 array with the encoded values \n",
    "- a counter indicating the words left\n",
    "- relative distance to the obstacles\n",
    "- current location of the agent\n",
    "\n",
    "\n",
    "**<h3><div>Policies</div></h3>**\n",
    "- Goal-oriented \"Biased policy\" - the agent only grabs if it is located at the same position as the defined task phoneme and heads towards the Goal.\n",
    "- Random policy - takes actions randomly if not phoneme at its position on the grid.\n",
    "- Combined policy  with  $ p <= \\epsilon $ explores, otherwise follows the biased policy.\n",
    "\n",
    "\n",
    "**<h3><div>Rewards</div></h3>**\n",
    "\n",
    "- -1 per each time step\n",
    "- -20 for hitting a moving obstacle \n",
    "- -10 for grabbing in an empty cell or hitting a wasll\n",
    "- -10 for grabbing a word with 'ʊ' sound\n",
    "- -20 for grabbing ʌ_pos and uː\n",
    "- 100 if grabbing the correct sound\n",
    "\n",
    "-  reaching the goal if all $ʊ$ were collected  $a\\times w_i$\n",
    "-  reaching the goal and $ʊ$ left  $ a \\times (w_i - ʊ_l)$\n",
    "-  if time step reached and the agent is not the goal on the  $ -a$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/INM707/task_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UFxxvDnE9ht",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mc4bd3drub2y",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "from collections import namedtuple, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed = 123\n",
    "rng = np.random.default_rng(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iUIuAWkFB-m",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2><div2>Section 1: The Environment</div2></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><div3>1.1 Phonemes list</div3></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = \"book took drum luck hush brush who tool new jury blush true through sue ball suit knew \" \\\n",
    "        \"fool loose lose pull room good boot look wolf rug foot sugar put dune hook doom cook \" \\\n",
    "        \"June cushion one could shoe woods bookshelf blue during rural noodles hush bug woman \" \\\n",
    "        \"football full would do too soon hood food pool you threw Lou two supper plumber publish \" \\\n",
    "        \"cup come \"\n",
    "\n",
    "phonemes = \"bʊk tʊk drʌm lʌk hʌʃ brʌʃ huː tuːl njuː ʤʊəri blʌʃ truː θruː sjuː bɔːl sjuːt njuː \" \\\n",
    "           \"fuːl luːs luːz pʊl ruːm gʊd buːt lʊk wʊlf rʌg fʊt ʃʊgə pʊt djuːn hʊk duːm kʊk ʤuːn \" \\\n",
    "           \"kʊʃən wʌn kʊd ʃuː wʊdz bʊkʃɛlf bluː djʊərɪŋ rʊərəl nuːdlz hʌʃ bʌg wʊmən fʊtbɔːl fʊl \" \\\n",
    "           \"wʊd duː tuː suːn hʊd fuːd puːl juː θruː luː tuː sʌpə plʌmə pʌblɪʃ kʌp kʌm \"\n",
    "\n",
    "spell_list = list(spell.split(\" \"))\n",
    "phonemes_list = list(phonemes.split(\" \"))\n",
    "\n",
    "phonetic_dict = dict(zip(phonemes_list, spell_list, ))\n",
    "\n",
    "# noinspection NonAsciiCharacters\n",
    "ʊ_sound = list()\n",
    "# noinspection NonAsciiCharacters\n",
    "uː_sound = list()\n",
    "# noinspection NonAsciiCharacters\n",
    "ʌ_sound = list()\n",
    "\n",
    "for pho in phonemes_list:\n",
    "    if 'ʊ' in pho:\n",
    "        ʊ_sound.append(phonetic_dict[pho])\n",
    "    elif 'uː' in pho:\n",
    "        uː_sound.append(phonetic_dict[pho])\n",
    "    elif 'ʌ' in pho:\n",
    "        ʌ_sound.append(phonetic_dict[pho])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><div3>1.1 Phonemes list</div3></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bI-43zARfUPY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# noinspection NonAsciiCharacters\n",
    "class Phonemes:\n",
    "\n",
    "    def __init__(self, size, action, **env_inf):\n",
    "        \"\"\"\n",
    "        param action: a napmedtuple with agent's actions\n",
    "        param size: is a tuple with number of column and raws\n",
    "        param env_inf: a dictionary containing informatio about the seed and task\n",
    "        \"\"\"\n",
    "        \n",
    "        self.size = size\n",
    "        self.grid = np.zeros(size)\n",
    "        self.up = action('up', 0, -1, 0)\n",
    "        self.down = action('down', 1, 1, 0)\n",
    "        self.left = action('left', 2, 0, -1)\n",
    "        self.right = action('right', 3, 0, 1)\n",
    "        self.grab = action('grab', 4, 0, 0)\n",
    "        self.seed = env_inf['seed']\n",
    "        self.task = env_inf['sound']\n",
    "        self.bound = 2 * np.sum(self.size) - 4\n",
    "        self.area = np.prod(self.size) - self.bound\n",
    "\n",
    "        # Boundaries\n",
    "        self.grid[0, :], self.grid[:, 0], self.grid[:, -1], self.grid[-1, :] = 7, 7, 7, 7\n",
    "\n",
    "        # Words to place on the grid on ly a third of the available area\n",
    "        self.num_words = self.area // 3\n",
    "\n",
    "        assert self.num_words >= 3, 'Increase the size of the environment'\n",
    "\n",
    "        self.obstacles = self.area // 9\n",
    "\n",
    "        # sum all objects in the environment, words, obstacles, goal and agent\n",
    "        self.total_objects = self.num_words + self.obstacles + 2  # goal + agent\n",
    "        self.total_agents = self.obstacles + 1  # learner agent\n",
    "\n",
    "        # Ramdomly choosing words\n",
    "        self.short_u = np.random.choice(ʊ_sound, self.num_words // 3)\n",
    "        self.open_middle_a = np.random.choice(ʌ_sound, self.num_words // 3)\n",
    "        self.long_u = np.random.choice(uː_sound, self.num_words // 3)\n",
    "        \n",
    "        self.agent_pos = None\n",
    "        self.ʊ_pos = None\n",
    "        self.ʌ_pos = None\n",
    "        self.uː_pos = None\n",
    "        self.obstacle_pos = None\n",
    "        self.goal_pos = None\n",
    "        self.time_step = 0\n",
    "        self.time_limit = round((self.area + self.obstacles) * 3)\n",
    "        self.dict_map_display = {0: '_', 1: '*', 2: 'ʊ', 3: 'ʌ', 4: 'u:', 5: 'A', 6: 'G', 7: 'X'}\n",
    "\n",
    "    def env_step(self, action, prints=True):\n",
    "        \"\"\"\n",
    "        This metods returns the observations, reward and boolean done\n",
    "        transitions to another position are checked\n",
    "        if the agent grab in a position with a word then\n",
    "        the item is remove from the list of words\n",
    "        after the agent action the obstacles are move and the observation is updated\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        \n",
    "        (x, y) = self.agent_pos\n",
    "        \n",
    "        if prints: \n",
    "            print('Agent position: {} |  Agent action: {} | Goal: {}'.format(self.agent_pos, action,\n",
    "                                                                        self.goal_pos))\n",
    "        reward = -1\n",
    "        self.time_step += 1\n",
    "\n",
    "        #############################\n",
    "        # Undertaking an action\n",
    "        #############################\n",
    "\n",
    "        if action == self.up.name:\n",
    "\n",
    "            self.agent_pos = (x + self.up.delta_x, y)\n",
    "\n",
    "            if self.agent_pos[0] < 1:\n",
    "                self.agent_pos = (x, y)\n",
    "                reward = -10\n",
    "\n",
    "            elif self.agent_pos in self.obstacle_pos:\n",
    "                self.agent_pos = (x, y)\n",
    "                reward = -20\n",
    "\n",
    "        elif action == self.down.name:\n",
    "            self.agent_pos = (x + self.down.delta_x, y)\n",
    "\n",
    "            if self.agent_pos[0] > self.size[0] - 2:\n",
    "                self.agent_pos = (x, y)\n",
    "                reward = -10\n",
    "\n",
    "            elif self.agent_pos in self.obstacle_pos:\n",
    "                self.agent_pos = (x, y)\n",
    "                reward = -20\n",
    "\n",
    "        elif action == self.left.name:\n",
    "\n",
    "            self.agent_pos = (x, y + self.left.delta_y)\n",
    "            if self.agent_pos[1] < 1:\n",
    "                self.agent_pos = (x, y)\n",
    "                reward = -10\n",
    "\n",
    "            elif self.agent_pos in self.obstacle_pos:\n",
    "                self.agent_pos = (x, y)\n",
    "                reward = -20\n",
    "\n",
    "        elif action == self.right.name:\n",
    "            self.agent_pos = (x, y + self.right.delta_y)\n",
    "\n",
    "            if self.agent_pos[1] > self.size[1] - 2:\n",
    "                self.agent_pos = (x, y)\n",
    "                reward = -10\n",
    "\n",
    "            elif self.agent_pos in self.obstacle_pos:\n",
    "                self.agent_pos = (x, y)\n",
    "                reward = -20\n",
    "                \n",
    "        elif action == self.grab.name and self.agent_pos in self.ʊ_pos:\n",
    "            # update list of items left\n",
    "            self.ʊ_pos.remove(self.agent_pos)\n",
    "            self.agent_pos = (x, y)\n",
    "\n",
    "            if self.task == 'short_u':\n",
    "                reward = 100\n",
    "            else:\n",
    "                reward = -10\n",
    "\n",
    "        elif action == self.grab.name and self.agent_pos in self.ʌ_pos:\n",
    "            # update list of items left\n",
    "            self.ʌ_pos.remove(self.agent_pos)\n",
    "            self.agent_pos = self.agent_pos\n",
    "\n",
    "            if self.task == 'middle_open':\n",
    "                reward = 100\n",
    "            else:\n",
    "                reward = -10\n",
    "\n",
    "        elif action == self.grab.name and self.agent_pos in self.uː_pos:\n",
    "\n",
    "            self.uː_pos.remove(self.agent_pos)\n",
    "            self.agent_pos = self.agent_pos\n",
    "            if self.task == 'long_u':\n",
    "                reward = 100\n",
    "            else:\n",
    "                reward = -10\n",
    "\n",
    "        elif action == self.grab.name and self.agent_pos not in (\n",
    "                self.ʊ_pos + self.ʌ_pos + self.uː_pos):\n",
    "            self.agent_pos = self.agent_pos\n",
    "            reward = -10\n",
    "\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        #############################\n",
    "        # Verifying terminal state\n",
    "        #############################\n",
    "\n",
    "        # Time limit reached\n",
    "        w = self.num_words//3 - len(self.ʊ_pos)\n",
    "        if self.time_step == self.time_limit and self.agent_pos != self.goal_pos:\n",
    "            done = True\n",
    "            w = self.num_words//3 - len(self.ʊ_pos)\n",
    "\n",
    "            if prints:\n",
    "                print('Episode done')\n",
    "                print('Last reward: {}'.format(reward))\n",
    "                print('Words with {} sound collected: {}'.format('ʊ', w))   \n",
    "\n",
    "        elif self.agent_pos == self.goal_pos and self.time_step <= self.time_limit:\n",
    "            done = True\n",
    "            \n",
    "            if len(self.ʊ_pos) == 0:\n",
    "                a = self.num_words // 3\n",
    "            elif len(self.ʊ_pos) > 0:\n",
    "                a = self.num_words//3 - len(self.ʊ_pos)\n",
    "                \n",
    "            else:\n",
    "                a = -1\n",
    "\n",
    "            reward = self.area * a\n",
    "\n",
    "            if prints:\n",
    "                print('Episode done')\n",
    "                print('Last reward: {}'.format(reward))\n",
    "                print('Words with {} sound collected: {}'.format('ʊ', w))                                           \n",
    "            \n",
    "        else:\n",
    "            obst_pos, obs_reward = self.move_obstacles()\n",
    "            reward = reward - obs_reward\n",
    "            if prints:\n",
    "                print('Step reward: {} | Obstacles positions: {}'. format(reward, obst_pos))\n",
    "            \n",
    "        observation = self.observe()\n",
    "\n",
    "        return observation, reward, done, self.time_step\n",
    "\n",
    "    def move_obstacles(self):\n",
    "        \"\"\"\n",
    "        This function moves randomly the obstacles in the grid and updates the list\n",
    "        of their position self.obstacle_pos for displaying\n",
    "        \"\"\"\n",
    "\n",
    "        directions = [self.up, self.down, self.left, self.right]\n",
    "        \n",
    "        obs_reward = 0\n",
    "\n",
    "        for i in range(len(self.obstacle_pos)):\n",
    "\n",
    "            new_pos = np.array(self.obstacle_pos[i])\n",
    "\n",
    "            (x, y) = new_pos\n",
    "            \n",
    "            idx = int(np.random.choice(4, 1))\n",
    "            action = directions[idx]\n",
    "\n",
    "            if action == self.up or action == self.down:\n",
    "\n",
    "                new_pos = (x + action.delta_x, y)\n",
    "\n",
    "                if new_pos[0] < 1:\n",
    "                    new_pos = (x, y)\n",
    "\n",
    "                elif new_pos[0] > self.size[0] - 2:\n",
    "                    new_pos = (x, y)\n",
    "\n",
    "                elif new_pos in self.obstacle_pos:\n",
    "                    new_pos = (x, y)\n",
    "\n",
    "                elif new_pos == self.agent_pos:\n",
    "                    new_pos = (x, y)\n",
    "                    obs_reward = 20\n",
    "\n",
    "                else:\n",
    "                    obs_reward = 0\n",
    "\n",
    "                self.obstacle_pos[i] = new_pos\n",
    "\n",
    "            else:\n",
    "\n",
    "                new_pos = (x, y + action.delta_y)\n",
    "\n",
    "                if new_pos[1] < 1:\n",
    "                    new_pos = (x, y)\n",
    "\n",
    "                elif new_pos[1] > self.size[1] - 2:\n",
    "                    new_pos = (x, y)\n",
    "\n",
    "                elif new_pos in self.obstacle_pos:\n",
    "                    new_pos = (x, y)\n",
    "\n",
    "                elif new_pos == self.agent_pos:\n",
    "                    new_pos = (x, y)\n",
    "                    obs_reward = 20\n",
    "\n",
    "                else:\n",
    "                    obs_reward = 0\n",
    "\n",
    "                self.obstacle_pos[i] = new_pos\n",
    "\n",
    "        return self.obstacle_pos, obs_reward\n",
    "\n",
    "    @staticmethod\n",
    "    def position_to_index(position, size):\n",
    "        \"\"\"\n",
    "        param position: x,y coordinates\n",
    "        return: coordinates index\n",
    "        \"\"\"\n",
    "        return np.ravel_multi_index(position, size)\n",
    "\n",
    "    def observe(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of the current observation of the environment\n",
    "        including distance to the goal, to the obsatcles and the words left\n",
    "        in the environment. The agent cannot see a word or the goal if an obstacle is\n",
    "        superimposed, but knows the location of the words.\n",
    "        \"\"\"\n",
    "        o = dict()\n",
    "\n",
    "        distance_to_obs = list()\n",
    "        distance_to_task = list()\n",
    "\n",
    "        # Distance to the obstacles\n",
    "        for pos in self.obstacle_pos:\n",
    "            distance_to_obs.append((np.array(pos) - np.array(self.agent_pos)))\n",
    "\n",
    "        # Distance to ʊ words\n",
    "        if self.task == 'short_u':\n",
    "            for pos in self.ʊ_pos:\n",
    "                distance_to_task.append((np.array(pos) - np.array(self.agent_pos)))\n",
    "        elif self.task == 'middle_open':\n",
    "            for pos in self.ʌ_pos:\n",
    "                distance_to_task.append((np.array(pos) - np.array(self.agent_pos)))\n",
    "        else:\n",
    "            for pos in self.uː_pos:\n",
    "                distance_to_task.append((np.array(pos) - np.array(self.agent_pos)))\n",
    "\n",
    "        o['obstacles'] = distance_to_obs\n",
    "        o['dist_goal'] = np.array(self.goal_pos) - np.array(self.agent_pos)\n",
    "        o['ʊ_pos'] = distance_to_task\n",
    "        o['ʊ_coords'] = self.ʊ_pos\n",
    "        o['agent_pos'] = self.agent_pos\n",
    "        o['pho_left'] = np.array((len(self.ʊ_pos), len(self.ʌ_pos), len(self.uː_pos)))\n",
    "\n",
    "        ob_rep, env_ob, _ = self.display()\n",
    "\n",
    "        # Agent surroundings\n",
    "        o['neigh'] = env_ob[self.agent_pos[0] - 1:\n",
    "                            self.agent_pos[0] + 2, self.agent_pos[1] - 1:\n",
    "                            self.agent_pos[1] + 2]\n",
    "\n",
    "        return o\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"\n",
    "        Displays the action of the agent and the location of the words, goal and obstacles\n",
    "        :return: string of the evironment, an array with agent observation (3X3) and array of\n",
    "        environment to render using sns.\n",
    "        \"\"\"\n",
    "\n",
    "        envir_rend = self.grid.copy()\n",
    "\n",
    "        envir_rend[self.goal_pos] = 6\n",
    "\n",
    "        for pos in self.ʊ_pos:\n",
    "            envir_rend[pos] = 2\n",
    "\n",
    "        for pos in self.ʌ_pos:\n",
    "            envir_rend[pos] = 3\n",
    "\n",
    "        for pos in self.uː_pos:\n",
    "            envir_rend[pos] = 4\n",
    "\n",
    "        for obs in self.obstacle_pos:\n",
    "            envir_rend[obs] = 1\n",
    "\n",
    "        env_ob = envir_rend.copy()\n",
    "\n",
    "        envir_rend[self.agent_pos] = 5\n",
    "\n",
    "        rend_grid = \"\"\n",
    "\n",
    "        for r in range(self.size[0]):\n",
    "\n",
    "            line = ''\n",
    "\n",
    "            for c in range(self.size[1]):\n",
    "                string_rend = self.dict_map_display[envir_rend[r, c]]\n",
    "\n",
    "                line += '{0:2}'.format(string_rend)\n",
    "\n",
    "            rend_grid += line + '\\n'\n",
    "\n",
    "        return rend_grid, env_ob, envir_rend\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Randomly places phonemes, obstacles, goal and agent\n",
    "        :return: observation of the environment\n",
    "        \"\"\"\n",
    "\n",
    "        self.time_step = 0\n",
    "\n",
    "        coord = list()\n",
    "\n",
    "        for r in range(1, self.size[0] - 1):\n",
    "            for c in range(1, self.size[1] - 1):\n",
    "                coord.append((r, c))\n",
    "\n",
    "        if self.seed:\n",
    "            rng.shuffle(coord)\n",
    "            \n",
    "        else:\n",
    "            np.random.shuffle(coord)\n",
    "\n",
    "        self.ʊ_pos = list()\n",
    "        self.uː_pos = list()\n",
    "        self.ʌ_pos = list()\n",
    "        self.obstacle_pos = list()\n",
    "\n",
    "        phonemes = self.num_words // 3\n",
    "\n",
    "        for phoneme in range(phonemes):\n",
    "            self.ʊ_pos.append(coord.pop())\n",
    "            self.uː_pos.append(coord.pop())\n",
    "            self.ʌ_pos.append(coord.pop())\n",
    "\n",
    "        for obs in range(self.obstacles):\n",
    "            self.obstacle_pos.append(coord.pop())\n",
    "\n",
    "        self.goal_pos = coord.pop()\n",
    "\n",
    "        self.agent_pos = coord.pop()\n",
    "\n",
    "        observation = self.observe()\n",
    "\n",
    "        return observation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rend_sns(env_array):\n",
    "    \"\"\"\n",
    "    Convert a numpy array to a sns heat map\n",
    "    :param env_array: an array representing the evironment/grid\n",
    "    :return: a heat map with of array\n",
    "    \"\"\"\n",
    "    \n",
    "    fig,ax = plt.subplots(1, figsize=(6,4))\n",
    "\n",
    "    # Colors for each of the unique items on the grid for the heatmap\n",
    "    cmap = ['#ffffd9', '#202603', '#c2e699', '#7fcdbb', '#1d91c0', '#2ac01d', '#f1dc18',\n",
    "            '#041f61']\n",
    "    items = len(np.unique(env_array))\n",
    "    sns.heatmap(env_array, linewidth=0.5, cmap=ListedColormap(cmap), ax=ax)\n",
    "    colorbar = ax.collections[0].colorbar\n",
    "    m = colorbar.vmax - colorbar.vmin\n",
    "    colorbar.set_ticks(\n",
    "        [colorbar.vmin + 0.5 * m/ items + m * i / items for i in range(items)])\n",
    "    colorbar.set_ticklabels(['empty', 'obstacle', 'ʊ', 'ʌ', 'u :', 'agent', 'goal', 'wall'])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><div>Section 2 : Policies</div></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biased_policy(observation, actions):\n",
    "    \"\"\"\n",
    "    This a goal oriented function, directs the agent towards the goal\n",
    "    only grabs when it is superimposed with the with phonem ʊ\n",
    "    :param actions: list of action to perform\n",
    "    :param observation:  adictionary with observation of the environment\n",
    "    :return: an action to be executed by the agent\n",
    "    \"\"\"\n",
    "    \n",
    "    coord = observation['dist_goal']\n",
    "    agent = observation['agent_pos']\n",
    "    obs = observation['neigh']\n",
    "    short_u = observation['ʊ_coords']\n",
    "\n",
    "    if agent in short_u:\n",
    "\n",
    "        action = actions[4]\n",
    "\n",
    "    elif 1 or 7 not in obs:\n",
    "\n",
    "        action = np.random.choice(actions[0:-1])\n",
    "\n",
    "    elif 7 not in obs[:, 1:] and 7 not in obs[1:, 1:]:\n",
    "        action = np.random.choice([actions[1], actions[3]])\n",
    "\n",
    "    elif 7 in obs[:, 2]:\n",
    "        action = actions[2]\n",
    "\n",
    "    elif 1 not in obs[:, 1:]:\n",
    "\n",
    "        action = np.random.choice([actions[0], actions[1], actions[3]])\n",
    "\n",
    "    elif coord[0] < 0 < coord[1]:\n",
    "\n",
    "        action = np.random.choice([actions[0], actions[3]])\n",
    "\n",
    "    # elif coord[0] > 0 and coord[1] > 0:\n",
    "    elif coord[0] > 0 < coord[1]:\n",
    "\n",
    "        action = np.random.choice([actions[1], actions[3]])\n",
    "\n",
    "    elif coord[0] > 0 > coord[1]:\n",
    "\n",
    "        action = np.random.choice([actions[1], actions[2]])\n",
    "\n",
    "    elif coord[0] < 0 > coord[1]:\n",
    "        action = np.random.choice([actions[1], actions[2]])\n",
    "\n",
    "    elif coord[0] == 0 and coord[1] < 0:\n",
    "        action = actions[2]\n",
    "\n",
    "    elif coord[0] == 0 and coord[1] > 0:\n",
    "\n",
    "        action = actions[3]\n",
    "\n",
    "    elif coord[0] > 0 and coord[1] == 0:\n",
    "        action = actions[1]\n",
    "\n",
    "    elif coord[0] < 0 and coord[1] == 0:\n",
    "        action = actions[0]\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(observation, actions):\n",
    "    \"\"\"\n",
    "    Chose a random action from a list of actions\n",
    "    :param observation:\n",
    "    :param actions:\n",
    "    :return: an action to be executed by the agent\n",
    "    \"\"\"  \n",
    "    agent = observation['agent_pos']\n",
    "    short_u = observation['ʊ_coords']\n",
    "\n",
    "    if agent in short_u:\n",
    "        action = actions[4]\n",
    "    else:\n",
    "        action = np.random.choice(actions)    \n",
    "    \n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_policy(observation, actions):\n",
    "    \"\"\"\n",
    "    Explore (epsilon) or follows a biased policy\n",
    "    :param observation:  a dictionary with observation of the environment\n",
    "    :param actions: list of action to perform\n",
    "    :return: an action to be executed by the agent\n",
    "    \"\"\"\n",
    "\n",
    "    epsilon  = 0.01\n",
    "    \n",
    "    prob = np.random.random()\n",
    "    \n",
    "    if prob <= epsilon:\n",
    "        \n",
    "        action = np.random.choice(actions)\n",
    "              \n",
    "    else:   \n",
    "        \n",
    "        action = biased_policy(observation, actions)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><div>Section 3. Running and Plotting Functions</div></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><div> 3.1 Running the experiment </div></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(environment, episode_stats, policy, pol_name='biased', number_of_episodes=100,\n",
    "                   display=True, rend_str=True, prints=True):\n",
    "    \"\"\"\n",
    "    Run the experiment\n",
    "    :param episode_stats:\n",
    "    :param environment: a method, the enviroment created\n",
    "    :param episode_stats:  anmed tuple to store the stats\n",
    "    :param policy: a funtion policy to take an action\n",
    "    :param pol_name: a string with name of the policy\n",
    "    :param number_of_episodes: number time the experiment runs\n",
    "    :param display: boolean show or not the environment on a heat map\n",
    "    :param rend_str: boolean show the environment in string characters\n",
    "    :param prints: boolean provides information about the game\n",
    "    :return: a namedtuple with statistics of the episode\n",
    "    \"\"\"\n",
    "\n",
    "    assert pol_name in ['biased', 'random',\n",
    "                        'combined'], 'Name of the policies: biased, random and combined'\n",
    "\n",
    "    actions = ['up', 'down', 'left', 'right', 'grab']\n",
    "    episode_reward = list()\n",
    "    episode_length = list()\n",
    "    episode_mean = list()\n",
    "    reward_std = list()\n",
    "\n",
    "    for _ in tqdm(range(number_of_episodes)):\n",
    "\n",
    "        reward_list = list()\n",
    "\n",
    "        # initialize state\n",
    "        observation = environment.reset()\n",
    "\n",
    "        # indicate terminal state\n",
    "        done = False\n",
    "        # log the accumulated regard\n",
    "        rewards = 0\n",
    "\n",
    "        # track time steps\n",
    "        time_step = 0\n",
    "\n",
    "        # repeat for each step of episode, until state is terminal\n",
    "        while not done:\n",
    "\n",
    "            # increase step counter - for display\n",
    "            time_step += 1\n",
    "\n",
    "            # choose action from state\n",
    "            action = policy(observation, actions)\n",
    "\n",
    "            # perform an action, observe, reward and done\n",
    "            next_observation, reward, done, steps = environment.env_step(action, prints=prints)\n",
    "\n",
    "            # observation <- next_observation\n",
    "            observation = next_observation\n",
    "\n",
    "            # accumulate reward\n",
    "            rewards += reward\n",
    "\n",
    "            reward_list.append(reward)\n",
    "\n",
    "            str_env, _, env_rend = environment.display()\n",
    "\n",
    "            if rend_str:\n",
    "                print(str_env)\n",
    "\n",
    "            if display:\n",
    "                print(\"Display on\")\n",
    "                rend_sns(env_rend)\n",
    "\n",
    "        reward_std.append(np.std(reward_list))\n",
    "        episode_reward.append(rewards)\n",
    "        episode_mean.append(np.mean(reward_list))\n",
    "        episode_length.append(time_step)\n",
    "\n",
    "    best_episode = np.argmax(episode_reward, axis=0)\n",
    "    print('Max reward: {} | Episode: {} | Steps: {} | Policy: {} '.format(\n",
    "        np.max(episode_reward), best_episode + 1, episode_length[best_episode], pol_name))\n",
    "\n",
    "    stats = episode_stats(length_episodes=np.array(episode_length),\n",
    "                          reward_episodes=np.array(episode_reward),\n",
    "                          episode_mean_reward=episode_mean, episode_std=np.array(reward_std))\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><div>3.2 Visualising Statistics</div></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_episodes_stats(stats, pol_name, episodes=None, smoothing_window=10, hideplot=False,\n",
    "                       env_dim=None):\n",
    "    \"\"\"\n",
    "    :param stats: a namedtuple containing the stats\n",
    "    :param pol_name: policy to train the agent\n",
    "    :param episodes: number of episodes run by the agent\n",
    "    :param smoothing_window: intiger, number of observations per eavh window\n",
    "    :param hideplot: boolean to display the plots\n",
    "    :param env_dim: string with the environment dimensions\n",
    "    :return: plots\n",
    "    Note: This code was adapted from Microsoft, Introduction to Reinforcement Learning.\n",
    "    \"\"\"\n",
    "\n",
    "    assert pol_name in ['biased', 'random',\n",
    "                        'combined'], 'Policies allowed: biased, random and combined'\n",
    "\n",
    "    figs_dir = os.path.join(os.getcwd(), 'plots', pol_name)\n",
    "    os.makedirs(figs_dir, exist_ok=True)\n",
    "    size = (7, 4)\n",
    "\n",
    "    # Plot the episode length over time\n",
    "    fig1 = plt.figure(figsize=size)\n",
    "    x = np.arange(1, episodes + 1)\n",
    "    plt.plot(x, stats.length_episodes, color='#0000B3')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Length\")\n",
    "    plt.title(\"Episode Length\")\n",
    "    plt.savefig(os.path.join(figs_dir, 'episodes_{}_{}_{}.png'.format(episodes, pol_name, env_dim)))\n",
    "    if hideplot:\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.rcParams.update({'font.size': 10})\n",
    "        plt.show()\n",
    "\n",
    "    # Plot the episode reward over time\n",
    "    fig2 = plt.figure(figsize=size)\n",
    "    rewards_smoothed = pd.Series(stats.reward_episodes).rolling(smoothing_window,\n",
    "                                                                min_periods=smoothing_window).mean()\n",
    "    plt.plot(x, rewards_smoothed, color='#0000B3')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Reward (Smoothed)\")\n",
    "    plt.title(\"Episode Reward over Time (Smoothed over window size {})\".format(smoothing_window))\n",
    "    plt.savefig(os.path.join(figs_dir, 'reward_{}_{}_{}.png'.format(episodes, pol_name, env_dim)))\n",
    "    if hideplot:\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.rcParams.update({'font.size': 10})\n",
    "    plt.show(fig2)\n",
    "\n",
    "    # Plot the episode mean reward per episode\n",
    "    fig3 = plt.figure(figsize=size)\n",
    "    mean_smoothed = pd.Series(stats.episode_mean_reward). \\\n",
    "        rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "    plt.plot(x, mean_smoothed, color='#0000B3')\n",
    "    plt.fill_between(x, mean_smoothed - stats.episode_std / 2,\n",
    "                     mean_smoothed + stats.episode_std / 2,\n",
    "                     color='#0000B3', alpha=0.2)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "    plt.title(\"Average Reward per Episode and std (Smoothed over window size {})\".format(\n",
    "        smoothing_window))\n",
    "    plt.savefig(\n",
    "        os.path.join(figs_dir, 'average_{}_policy_{}_{}.png'.format(episodes, pol_name, env_dim)))\n",
    "    if hideplot:\n",
    "        plt.close(fig3)\n",
    "    else:\n",
    "        plt.rcParams.update({'font.size': 10})\n",
    "    plt.show(fig3)\n",
    "\n",
    "    return fig1, fig2, fig3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><div>Section 4. Runing the experiment</div></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><div> 4.1 Visualise the initial state of the environment </div></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tupe to store call the actions\n",
    "Action = namedtuple('Action', 'name index delta_x delta_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tuple to store the statistics\n",
    "episode_stats = namedtuple(\"Stats\",[\"length_episodes\", \"reward_episodes\", \"episode_mean_reward\", \"episode_std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the environment and select the task\n",
    "env_info = {'seed': True, 'sound': 'short_u'}\n",
    "\n",
    "# Creates the environment\n",
    "size = (7,7)\n",
    "my_env= Phonemes(size, Action, **env_info)\n",
    "\n",
    "obs = my_env.reset()\n",
    "str_dis, _ , envir_rend= my_env.display()\n",
    "\n",
    "# Information about the environment\n",
    "print('Available area: {} | Total objects on the grid: {}'.format(my_env.area, my_env.total_objects))\n",
    "print('Total number of agents on the grid: {}, {} - learning agent and {} moving obstacle(s)'.\n",
    "      format(my_env.obstacles + 1, 1, my_env.obstacles))\n",
    "print('Mission: learn {} sound'.format(my_env.task))\n",
    "print('Words on the grid: ', my_env.short_u, my_env.open_middle_a, my_env.long_u, sep='\\n')\n",
    "print('Word(s) to find: ', my_env.short_u)\n",
    "\n",
    "# Renders the environment on a heatmap\n",
    "print(str_dis)\n",
    "rend_sns(envir_rend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "pol_name = 'combined'\n",
    "stats_log = run_experiment(my_env, episode_stats, combined_policy, pol_name, number_of_episodes=num_episodes, \n",
    "                           display=False, prints=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = plot_episodes_stats(stats_log, pol_name, episodes=num_episodes, smoothing_window=1, hideplot=False, env_dim= '5x5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><div>5. Comparing policies  with different size of evironments</div></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_per_env = 50\n",
    "envs_per_size = 5\n",
    "start, end, step = 5, 30, 5\n",
    "\n",
    "\n",
    "pol_names = ['biased', 'random', 'combined']\n",
    "\n",
    "pol_dict = defaultdict(list)\n",
    "\n",
    "for i, policy in tqdm(enumerate([biased_policy, random_policy, combined_policy])):\n",
    "    \n",
    "    pol_name = pol_names[i]\n",
    "    \n",
    "    aver_reward = list ()\n",
    "    std_reward = list ()\n",
    "    \n",
    "    for size_env in range(start, end+1, step):\n",
    "\n",
    "        cum_reward = list ()\n",
    "\n",
    "        for _ in range(envs_per_size):\n",
    "\n",
    "            my_env = Phonemes((size_env, size_env), Action, **env_info)\n",
    "        \n",
    "            exp_stats = run_experiment(my_env, episode_stats, policy, pol_name, episodes_per_env, \n",
    "                                       display=False, rend_str=False, prints=False)\n",
    "            \n",
    "            cum_reward.append(exp_stats.reward_episodes)\n",
    "\n",
    "        # Environments stats    \n",
    "        aver_reward.append(np.mean(cum_reward))\n",
    "        std_reward.append(np.std(cum_reward))\n",
    "\n",
    "    pol_dict[pol_name].append([np.asarray(aver_reward), np.asarray(std_reward)])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_params = {'biased': {'label': 'biased_policy', 'color':'r'}, \n",
    "              'random': {'label': 'random_policy', 'color':'b'},\n",
    "              'combined': {'label': 'combined_policy', 'color':'g'}}\n",
    "\n",
    "fig_dir = os.path.join(os.getcwd(), 'plots')\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "fig = plt.figure(figsize=(7,4))\n",
    "\n",
    "for k, v in pol_dict.items():\n",
    "    for m, s in v:\n",
    "        plt.plot(range(start, end+1, step), m, 'o' + plot_params[k]['color'])\n",
    "        plt.plot(range(start, end+1, step), m, color = plot_params[k]['color'], \n",
    "              label = plot_params[k]['label'] )\n",
    "        plt.fill_between(range(start,end+1, step), m - s/2, m + s/2, color=plot_params[k]['color'], alpha=0.2)\n",
    "plt.xlabel('Environment size')\n",
    "plt.ylabel('Average reward')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(fig_dir, 'env_comparison_{}_ep.png'.format(episodes_per_env)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "T1_Environment_phoneme_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
