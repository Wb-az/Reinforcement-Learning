{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMAGNoZ3R0MT"
   },
   "source": [
    "<h1><div style=\"text-align: center;\"> Reinforcement Learning </div></h1>\n",
    "<h2><div style=\"text-align: center;\"> Assigment: Q-learning agent with $\\epsilon$ greedy policy </div></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 869,
     "status": "ok",
     "timestamp": 1617874991906,
     "user": {
      "displayName": "Aze Gln",
      "photoUrl": "",
      "userId": "12590864696663313384"
     },
     "user_tz": -60
    },
    "id": "Rp5kdSX0JN2Z",
    "outputId": "730060ee-bfbb-414d-f9a0-a72cfef2fdb8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/INM707/task_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pF-z6VW-Vi1Q"
   },
   "source": [
    "<h2><div> Task objectives </div></h2> \n",
    "\n",
    "- Implement an agent that uses bootstrapping \"Q-learning\" in a MDP.\n",
    "- Train and evaluate the q-agent with an $\\epsilon-greedy$ policy in the Phoneme environment.\n",
    "- Visualise the effect of the training parameter on the learning results.\n",
    "\n",
    "We updated the number of obstacled and the rewards of Phoneme environment. We want to motivate the agent to search rather than only heading to the goal.\n",
    "\n",
    "- The obstacles move in the same direction as the agent\n",
    "- The number of obstacles on the grid is:\n",
    "\n",
    "$ o = \\lfloor \\frac{a}{10} \\rfloor $\n",
    "\n",
    "- The three modified rewards are: \n",
    "1. Reaching the time limit and $ʊ$ collected:  $a\\times w_i$\n",
    "2. Reaching the goal and $ʊ$ left: $ a \\times (w_i - ʊ_l - \\lfloor \\frac{a}{3} \\rfloor)$\n",
    "3. Reaching the goal with not words: $ -a \\times 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wo2NCpI6I9Iu"
   },
   "source": [
    "<h2><div>Libraries</div></h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1617874997419,
     "user": {
      "displayName": "Aze Gln",
      "photoUrl": "",
      "userId": "12590864696663313384"
     },
     "user_tz": -60
    },
    "id": "Og8h24MjR0Mf"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from matplotlib import style\n",
    "from collections import defaultdict, namedtuple\n",
    "from plotting import plot_episodes_stats, rend_sns\n",
    "from phonemes import Phonemes, Action \n",
    "rng=np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tASiuuBJI0aA"
   },
   "source": [
    "<h2><div>1. Q-learning Methods and Functions</div></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><div> 1.1 Implementing the Q-learning algorithm - Method</div></h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1035,
     "status": "ok",
     "timestamp": 1617875001336,
     "user": {
      "displayName": "Aze Gln",
      "photoUrl": "",
      "userId": "12590864696663313384"
     },
     "user_tz": -60
    },
    "id": "XN3AZCutR0Mg"
   },
   "outputs": [],
   "source": [
    "# noinspection NonAsciiCharacters\n",
    "\n",
    "class Qlagent(object):\n",
    "    \"\"\"\n",
    "    Learn by using the Q(s,a) updates\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, **agent_info):\n",
    "\n",
    "        self.action_list = ['up', 'down', 'left', 'right', 'grab']\n",
    "\n",
    "        self.actions = dict(map(reversed, enumerate(self.action_list)))\n",
    "        self.q_val = defaultdict(\n",
    "            lambda: {a: 0 for a in self.action_list})  # Optimistic initilisation\n",
    "        self.env = env\n",
    "        self.epsilon = agent_info['epsilon']  # Exploration rate\n",
    "        self.epsilon_init = agent_info['epsilon']  # Exploration rate\n",
    "        self.decay = agent_info['decay']  # Dicrease the level of exploration\n",
    "        self.gamma = agent_info['gamma']  # Discount factor\n",
    "        self.alpha = agent_info['alpha']  # learning rate\n",
    "        self.min_epsilon = agent_info['min_epsilon']\n",
    "\n",
    "    def state(self, obs):\n",
    "        \"\"\"\n",
    "        Converts the dictionary of observations into a string\n",
    "        :param obs: a dictionary with information about the environment\n",
    "        :return:  a string of the  agent's state\n",
    "        \"\"\"\n",
    "        \n",
    "        agent = np.array(obs['agent_pos'])\n",
    "        neigh = obs['neigh']\n",
    "        phon = obs['ʊ_pos']\n",
    "        pho_coord = np.array(obs['ʊ_coords'])\n",
    "        goal = np.array(obs['dist_goal'])\n",
    "        obstacles = obs['obstacles']\n",
    "        open_a = np.array(obs['ʌ_coords'])\n",
    "        long_u = np.array(obs['u:_coords'])\n",
    "\n",
    "        s = (agent, neigh, obstacles, phon, pho_coord, open_a, long_u, goal)\n",
    "    \n",
    "        return self.state_to_string(s)\n",
    "\n",
    "    @staticmethod\n",
    "    def state_to_string(state):\n",
    "        \"\"\"\n",
    "        :param state: a tuple with observations\n",
    "        :return a string of the agent's state\n",
    "        \"\"\"\n",
    "        \n",
    "        my_str = ''\n",
    "\n",
    "        for item in state:\n",
    "\n",
    "            if np.isscalar(item):\n",
    "                my_str += str(item) + ' '\n",
    "\n",
    "            elif isinstance(item, np.ndarray):\n",
    "\n",
    "                for i in item.astype(str):\n",
    "                    my_str += ' '.join(i) + ' '\n",
    "\n",
    "            else:\n",
    "                for i in item:\n",
    "                    if isinstance(i, np.ndarray):\n",
    "                        my_str += ' '.join(i.astype(str)) + ' '\n",
    "                    else:\n",
    "                        my_str += str(i)\n",
    " \n",
    "        return my_str\n",
    "\n",
    "    def take_action(self, obs):\n",
    "        \"\"\"\n",
    "        :param obs: a dictionary with information of the environment\n",
    "        :return: Return an action following an epsilon gredy policy\n",
    "        \"\"\"\n",
    "\n",
    "        s = self.state(obs)\n",
    "\n",
    "        rand_p = rng.random()\n",
    "\n",
    "        if rand_p < self.epsilon:\n",
    "\n",
    "            action = np.random.choice(self.action_list)\n",
    "\n",
    "        else:\n",
    "\n",
    "            action = self.argmax_act(s)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def argmax_act(self, s):\n",
    "        \"\"\"\n",
    "        :param s: a string contain the current state of the agent\n",
    "        :return:  Returns the best action by breaking ties.\n",
    "        \"\"\"\n",
    "        max_val = max([self.q_val[s][a] for a in self.action_list])\n",
    "        max_actions = [a for a in self.action_list if self.q_val[s][a] == max_val]\n",
    "\n",
    "        return np.random.choice(max_actions)\n",
    "\n",
    "    def update_epsilon(self):\n",
    "\n",
    "        self.epsilon *= self.decay\n",
    "        if self.epsilon < self.min_epsilon:\n",
    "            self.epsilon = self.min_epsilon\n",
    "\n",
    "    def update_qval(self, state1, state2, action, reward):\n",
    "        \"\"\"\n",
    "        Return the estimated Q-value difference between the estimate and target state action values\n",
    "        :param state1: a string with the previous state\n",
    "        :param state2: a string with the current state\n",
    "        :param action: the action taken on the previous state\n",
    "        :param reward: the reward from the previous state-action par\n",
    "        :return: None \n",
    "        \"\"\"\n",
    "\n",
    "        prev_s = self.state(state1)\n",
    "\n",
    "        current_s = self.state(state2)\n",
    "\n",
    "        predict = self.q_val[prev_s][action]\n",
    "\n",
    "        td_target = reward + self.gamma * max([self.q_val[current_s][a] for a in self.action_list])\n",
    "\n",
    "        # Update q-values\n",
    "        self.q_val[prev_s][action] += self.alpha * (td_target - predict)\n",
    "                        \n",
    "    def game_end(self, state1, action, reward):\n",
    "        \"\"\"\n",
    "        Compute the last update when agent terminates.\n",
    "        :param state1: a string with the previous state\n",
    "        :param action: the action taken on the previous state\n",
    "        :param reward: the reward from the previous state-action par\n",
    "        :return: None \n",
    "        \"\"\"\n",
    "       \n",
    "        prev_s = self.state(state1)\n",
    "\n",
    "        # Update q-values\n",
    "        self.q_val[prev_s][action] += self.alpha * (reward - self.q_val[prev_s][action])\n",
    "        \n",
    "    def reset_q(self):\n",
    "        \"\"\"\n",
    "        :return: reset the q_values to \n",
    "        \"\"\"\n",
    "        return self.q_val.clear()\n",
    "\n",
    "    def reset_eps(self):\n",
    "        \"\"\"\n",
    "        :return: reset the epsilon to its initial value\n",
    "        \"\"\"\n",
    "        return self.epsilon_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><div> 1.2 Running Q-learning experiments </div></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 669,
     "status": "ok",
     "timestamp": 1617875021855,
     "user": {
      "displayName": "Aze Gln",
      "photoUrl": "",
      "userId": "12590864696663313384"
     },
     "user_tz": -60
    },
    "id": "m_42dMnhR0Mi"
   },
   "outputs": [],
   "source": [
    "def qlearning_exp(environment, q_agent, stats_log, reset=False, **exp_params):\n",
    "    \"\"\"\n",
    "    :param environment: an numpy array\n",
    "    :param q_agent: a method that implemets the q-learning algorithm\n",
    "    :param stats_log: a named tuple to log the episode stats\n",
    "    :param exp_params: a dictionary with parameter to run the experiment - episodes, start decay\n",
    "    :return: qvalues and statistics\n",
    "    \"\"\"\n",
    "    short_u = environment.num_words//3\n",
    "    collected = list()\n",
    "    acting = list()\n",
    "    exp_reward = np.zeros(exp_params['episodes'])\n",
    "    exp_length = np.zeros(exp_params['episodes'])\n",
    "    exp_mean = np.zeros(exp_params['episodes'])\n",
    "    exp_var = np.zeros(exp_params['episodes'])\n",
    "    show_stats = exp_params['show_stats']\n",
    "    update = exp_params['update']\n",
    "    start_decay = exp_params['decay_from']\n",
    "\n",
    "    for episode in tqdm(range(0, exp_params['episodes'])):\n",
    "\n",
    "        reward_list = list()\n",
    "\n",
    "        # initialize state\n",
    "        observation = environment.reset()\n",
    "\n",
    "        # indicate terminal state\n",
    "        done = False\n",
    "\n",
    "        # loops until the state/observation is terminal\n",
    "        while not done:\n",
    "\n",
    "            # choose action from state using policy derived from Q\n",
    "            action = q_agent.take_action(observation)\n",
    "\n",
    "            acting.append(action)  # for exploration\n",
    "\n",
    "            # perform an action, observe, give a reward and indicate if terminal\n",
    "            next_observation, reward, done, steps = environment.env_step(action, prints=False)\n",
    "\n",
    "            exp_reward[episode] += reward  # accumulated reward\n",
    "\n",
    "            reward_list.append(reward)\n",
    "\n",
    "            # agent learn (Q-Learning update)\n",
    "            q_agent.update_qval(observation, next_observation, action, reward)\n",
    "\n",
    "            # observation <- next observation\n",
    "            observation = next_observation\n",
    "\n",
    "            if update == 'step' and episode + 1 >= start_decay:\n",
    "                q_agent.update_epsilon()  # updates epsilon per step\n",
    "\n",
    "        # agent learn (Q-Learning last update)\n",
    "        q_agent.game_end(observation, action, reward)\n",
    "        \n",
    "        if update == 'episode' and episode + 1 >= start_decay:\n",
    "            q_agent.update_epsilon() \n",
    "                 \n",
    "        exp_length[episode] += steps  # Store steps\n",
    "        exp_mean[episode] += np.mean(reward_list)  # average reward per episode\n",
    "        exp_var[episode] += np.std(reward_list)  # std of reward per episode\n",
    "\n",
    "        # Display environment\n",
    "        if exp_params['display']:\n",
    "            rend_str, _, rend_arr = environment.display()\n",
    "            print(rend_str)\n",
    "\n",
    "        collected.append(short_u - len(environment.ʊ_pos))\n",
    "        \n",
    "        if show_stats:\n",
    "            print('ʊ collected: {}'.format(collected[-1]))\n",
    "            print('ʊ in left in the environment: {}'.format(len(environment.ʊ_pos)))\n",
    "            print('Episode: {} | epsilon: {:.5f} | reward: {} |'.format(episode + 1, q_agent.epsilon,\n",
    "                                                                    exp_reward[episode]))\n",
    "    stats = stats_log(length_episodes=exp_length, reward_episodes=exp_reward, \n",
    "                      episode_mean_reward=exp_mean, episode_std=exp_var, \n",
    "                      words_collected=collected)\n",
    "    exp_std = np.std(exp_reward)\n",
    "    exp_total_mean = np.mean(exp_reward)\n",
    "\n",
    "    q_dict = q_agent.q_val\n",
    "    \n",
    "    if reset:\n",
    "    \n",
    "        q_agent.reset_q()\n",
    "    \n",
    "    return q_dict, exp_std, exp_total_mean, stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><div> 1.3 Running optimality with stored q-values </div></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimality(environment, q_dict, stats_log):\n",
    "    \"\"\"\n",
    "    Runs optimality with q-values from previous experiencesaved\n",
    "    :param environment: an numpy array\n",
    "    :param q_dict: a dictionary with stored q-values\n",
    "    :param stats_log: a named tuple to log the episode stats\n",
    "    :return: uddated q-values and statistics\n",
    "    \"\"\"\n",
    "\n",
    "    collected = list()\n",
    "    acting = list()\n",
    "    exp_reward = np.zeros(exp_params['episodes'])\n",
    "    exp_length = np.zeros(exp_params['episodes'])\n",
    "    exp_mean = np.zeros(exp_params['episodes'])\n",
    "    exp_var = np.zeros(exp_params['episodes'])\n",
    "    show_stats = exp_params['show_stats']\n",
    "    update = exp_params['update']\n",
    "    start_decay = exp_params['decay_from']\n",
    "    q_agent.q_val = q_dict\n",
    "\n",
    "    for episode in tqdm(range(0, exp_params['episodes'])):\n",
    "\n",
    "        reward_list = list()\n",
    "\n",
    "        # initialize state\n",
    "        observation = environment.reset()\n",
    "        short_u = len(environment.short_u)\n",
    "\n",
    "        # indicate terminal state\n",
    "        done = False\n",
    "\n",
    "        # loops until the state/observation is terminal\n",
    "        while not done:\n",
    "\n",
    "            # choose action from state using policy derived from Q\n",
    "            action = q_agent.take_action(observation)\n",
    "\n",
    "            acting.append(action)  # for exploration\n",
    "\n",
    "            # perform an action, observe, give a reward and indicate if terminal\n",
    "            next_observation, reward, done, steps = environment.env_step(action, prints=False)\n",
    "\n",
    "            exp_reward[episode] += reward  # accumulated reward\n",
    "\n",
    "            reward_list.append(reward)\n",
    "\n",
    "            # agent learn (Q-Learning update)\n",
    "            q_agent.update_qval(observation, next_observation, action, reward)\n",
    "\n",
    "            # observation <- next observation\n",
    "            observation = next_observation\n",
    "\n",
    "            if update == 'step' and episode + 1 >= start_decay:\n",
    "                q_agent.update_epsilon()  # updates epsilon per step\n",
    "\n",
    "        # agent learn (Q-Learning last update)\n",
    "        q_agent.game_end(observation, action, reward)\n",
    "\n",
    "        if update == 'episode' and episode + 1 >= start_decay:\n",
    "            q_agent.update_epsilon() \n",
    "             \n",
    "            \n",
    "        exp_length[episode] += steps  # Store steps\n",
    "        exp_mean[episode] += np.mean(reward_list)  # average reward per episode\n",
    "        exp_var[episode] += np.std(reward_list)  # std of reward per episode\n",
    "\n",
    "        # Display environment\n",
    "        if exp_params['display']:\n",
    "            rend_str, _, rend_arr = environment.display()\n",
    "            print(rend_str)\n",
    "\n",
    "        collected.append(short_u - len(environment.ʊ_pos))\n",
    "        \n",
    "        if show_stats:\n",
    "            print('ʊ in left in the environment: {}'.format(len(environment.ʊ_pos)))\n",
    "            print('Episode: {} | epsilon: {} | reward: {} |'.format(episode + 1, \n",
    "                                                q_agent.epsilon, exp_reward[episode]))\n",
    "            print('q_values: {}'.format(len(q_agent.q_val)))\n",
    "        \n",
    "\n",
    "    stats = stats_log(length_episodes=exp_length, reward_episodes=exp_reward, \n",
    "                      episode_mean_reward=exp_mean, episode_std=exp_var, \n",
    "                      words_collected=collected)\n",
    "    exp_std = np.std(exp_reward)\n",
    "    exp_total_mean = np.mean(exp_reward)\n",
    "\n",
    "    \n",
    "    return q_dict, exp_std, exp_total_mean, stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><div>2. Environment and Agent's parameters initilisation</div></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8ussEvLIdNU"
   },
   "source": [
    "<h3><div>2.1 Environment initialisation</div></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1617875003936,
     "user": {
      "displayName": "Aze Gln",
      "photoUrl": "",
      "userId": "12590864696663313384"
     },
     "user_tz": -60
    },
    "id": "VAXhwv7PR0Mh",
    "outputId": "5cbdbdda-a2b9-4d4e-eafb-3b0639c1fc3f"
   },
   "outputs": [],
   "source": [
    "env_info = {'seed':True, 'sound': 'short_u'}\n",
    "\n",
    "my_env= Phonemes((10,10), Action, **env_info)\n",
    "\n",
    "my_env.reset()\n",
    "str_dis, _ , envir_rend= my_env.display()\n",
    "\n",
    "# Information about the environment\n",
    "print('Available area: {} | Total objects on the grid: {}'.format(my_env.area, my_env.total_objects))\n",
    "print('Total number of agents on the grid: {}, {} - learning agent and {} moving obstacle(s)'.\n",
    "      format(my_env.obstacles + 1, 1, my_env.obstacles))\n",
    "print('Mission: learn the words with phonetic sound {}'.format('ʊ'))\n",
    "print('Word(s) to find: {}'.format(my_env.short_u))\n",
    "\n",
    "# Renders the environment on a heatmap\n",
    "rend_sns(envir_rend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8DstdxfJMVL"
   },
   "source": [
    "<h3><div>2.2 Agent Initilisation</div></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 429,
     "status": "ok",
     "timestamp": 1617875033736,
     "user": {
      "displayName": "Aze Gln",
      "photoUrl": "",
      "userId": "12590864696663313384"
     },
     "user_tz": -60
    },
    "id": "5YVhXPg4R0Mh"
   },
   "outputs": [],
   "source": [
    "agent_info= {'epsilon': 0.4, 'gamma': 0.95, 'alpha': 0.1, 'decay':0.9998, 'min_epsilon': 0.01}\n",
    "q_agent = Qlagent(my_env, **agent_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tuple to store the statistics\n",
    "episode_stats = namedtuple('Stats',['length_episodes', 'reward_episodes', 'episode_mean_reward', 'episode_std', 'words_collected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 418,
     "status": "ok",
     "timestamp": 1617875035158,
     "user": {
      "displayName": "Aze Gln",
      "photoUrl": "",
      "userId": "12590864696663313384"
     },
     "user_tz": -60
    },
    "id": "RVYU1avxR0Mj"
   },
   "outputs": [],
   "source": [
    "exp_params = {'episodes':50000, 'display': False, 'update':'episode', 'decay_from':100, 'show_stats':True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8T0a_MdCJvfP"
   },
   "source": [
    "<h2><div>Section 3. Running experiments and visualising results</div></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><div>3.1 Running the experiment</div></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1755463,
     "status": "ok",
     "timestamp": 1617876791665,
     "user": {
      "displayName": "Aze Gln",
      "photoUrl": "",
      "userId": "12590864696663313384"
     },
     "user_tz": -60
    },
    "id": "FeTlKY-YR0Mj",
    "outputId": "8b4c8f0b-71a2-4e08-d971-2f544901612c"
   },
   "outputs": [],
   "source": [
    "q_dict, exp_std, exp_mean, stats= qlearning_exp(my_env, q_agent, episode_stats, reset=True,\n",
    "                                                                 **exp_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><div>3.2 Saving results </div></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = stats._asdict()\n",
    "data_dir = os.path.join(os.getcwd(), 'results')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "name = str(exp_params['episodes']) + '_' + str(agent_info['alpha']) + str(agent_info['epsilon']) + '.npz'\n",
    "np.savez(os.path.join(data_dir, name), data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4u-6esRtR0Mo"
   },
   "source": [
    "<h3><div>3.3 Visualising Statistics</div></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = plot_episodes_stats(stats, episodes = exp_params['episodes'], smoothing_window=100, hideplot=False, env_dim=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmax(stats.reward_episodes)\n",
    "print('Maximum reward: {} | Episode: {} | Collected: {} | Steps {}'.format(max(stats.reward_episodes),\n",
    "                            np.argmax(stats.reward_episodes) + 1, stats.words_collected[idx],\n",
    "                            stats.length_episodes[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episodes_stats(stats, episodes = exp_params['episodes'], smoothing_window=750, hideplot=False, env_dim=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_params = {'episodes':50000, 'display': False, 'update':'episode', 'decay_from':100, 'show_stats':True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><div>3.4 Parameters search</div></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2225887,
     "status": "ok",
     "timestamp": 1617839904586,
     "user": {
      "displayName": "Aze Gln",
      "photoUrl": "",
      "userId": "12590864696663313384"
     },
     "user_tz": -60
    },
    "id": "TvvZk5mwmr75",
    "outputId": "b4ca09c2-f72d-40c4-f419-986c178d3da0"
   },
   "outputs": [],
   "source": [
    "alphas =[0.01, 0.1, 0.5]\n",
    "epsilons = [0.01, 0.1, 0.5]\n",
    "\n",
    "exp_params = {'episodes': 60000, 'display': False, 'update':'episode', 'decay_from': 1, 'show_stats':False}\n",
    "agent_info['min_epsilon']=0.001\n",
    "fname = 'comp_' + str(exp_params['episodes']) + '_.npz'\n",
    "\n",
    "comp_dict = dict()\n",
    "for e in epsilons:\n",
    "    for a in alphas:\n",
    "        q_agent.alpha = a\n",
    "        q_agent.epsilon = e\n",
    "        q_dict, exp_std, exp_mean, stats = qlearning_exp(my_env, q_agent, episode_stats,reset=True, **exp_params)\n",
    "        comp_dict[(e,a)]= stats.reward_episodes\n",
    "                \n",
    "np.savez(os.path.join(data_dir, fname), comp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><div>3.5 Visualise parameter search</div></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_params = ['r', 'b', 'g', 'm', 'c', 'y', 'k', 'orange','sienna'] \n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "step = 100\n",
    "\n",
    "for i, (k,v) in enumerate(comp_dict.items()):\n",
    "    m = np.array([np.mean(v[ii:ii+step]) for ii in range(0, len(v), step)])\n",
    "    s = np.array([np.std(v[ii:ii+step]) for ii in range(0, len(v), step)])\n",
    "    plt.plot(range(1, len(v)+1, step), m, color=plot_params[i], label='e={},a={}'.format(str(k[0]),str(k[1])))\n",
    "    plt.fill_between(range(1, len(v), step), m - s/2, m + s/2, color=plot_params[i], alpha=0.2)\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('average reward')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join('./plots', 'ql_comparison_{}_{}.png'.format(exp_params['episodes'], '10x10')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><div>4. Optimality test </div></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_params = {'episodes':50000, 'display': False, 'update':'episode', 'decay_from':100, 'show_stats':True}\n",
    "agent_info['min_epsilon']= 0.1\n",
    "agent_info['epsilon']= 0.001,\n",
    "q_dict_op, exp_std, exp_total_mean, op_stats = optimality(my_env, q_dict, episode_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episodes_stats(op_stats, episodes = exp_params['episodes'], smoothing_window=50, hideplot=False, env_dim=(10,10))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "T2_Qlearning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
